{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Chain"
      ],
      "metadata": {
        "id": "utpwgoWcWJLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langchain expression Language (LCEL)\n",
        "\n",
        "LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together\n",
        "\n"
      ],
      "metadata": {
        "id": "wjN0juQzlCyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "تو چین ها یه پایپلاین رو داریم میسازیم که بگیم چه طوریکه کار کنه برخلاف ایجنت ها"
      ],
      "metadata": {
        "id": "Ko8XLqbClGqK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abBsXCwwWA1f",
        "outputId": "9a243618-2e71-4239-bc3d-46f46f376483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.6/249.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q  langchain-cohere langchain_community langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "cohere_api_key =\"5pZ8wqLJIHRupGTVqBMgm0YOf5e4OQxVqz29hzU8\""
      ],
      "metadata": {
        "id": "Ntb3XO5EWcGz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_cohere import ChatCohere\n",
        "llm = ChatCohere(cohere_api_key=cohere_api_key)\n"
      ],
      "metadata": {
        "id": "UweZzmcrb6Sz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "chat = ChatCohere(cohere_api_key=cohere_api_key)\n",
        "messages = [HumanMessage(content=\"تیم پرسپولیس ایران کیست؟لطفا خلاصه بگو و در یک جمله\")]\n",
        "chat.invoke(messages).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "InH6l1XkYZ_i",
        "outputId": "cafe2d94-4f3b-4e5e-8cd1-2907e5fea960"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'تیم پرسپولیس، یکی از باشگاه\\u200cهای فوتبال محبوب و پرطرفدار ایران است که در شهر تهران مستقر است و سابقه\\u200cای طولانی در فوتبال این کشور دارد.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "llm.invoke(\"تیم پرسپولیس ایران کیست؟ لطفا خلاصه بگو و در یک جمله\").content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MklcwqzlXDY7",
        "outputId": "aa2f41b8-746d-4c0b-e483-1dc982270607"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'تیم پرسپولیس یک باشگاه فوتبال ایرانی است که در شهر تهران مستقر است و یکی از پرطرفدارترین و موفق\\u200cترین تیم\\u200cهای فوتبال در ایران به شمار می\\u200cرود.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ساده ترین Chain\n",
        "LLMChain"
      ],
      "metadata": {
        "id": "7Xi_YbCrbMhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "# from langchain.llms import OpenAI\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n"
      ],
      "metadata": {
        "id": "nncdKAsAY7JC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate.from_template(\"Do you know the well-known (please say the most important and the best) football team of the country? Please say two sentences. the name of country is: {the_name_of_country}?\")\n",
        "# llm = ChatOpenAI(openai_api_key=openai_api_key,model='gpt-4o-mini' ,temperature=0)\n",
        "llm = ChatCohere(cohere_api_key=cohere_api_key)\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "XfdYQtkkbg7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ee1037-195d-45ad-84d2-75c140104f5a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-91d3aadb79f2>:4: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(prompt.format(the_name_of_country='Iran')).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ddngdFlLd-T5",
        "outputId": "0ff5b2c4-f552-4641-e9a3-b2d7fa954bf3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Persepolis Football Club is one of the most popular and successful football teams in Iran, having won multiple domestic titles and enjoying a large fan base. The club's success and its impact on Iranian football culture have made it a well-recognized name not only in the country but also across the region.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({'the_name_of_country':\"Iran\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBm-HLyob-F3",
        "outputId": "e640b548-8a6a-4159-a48a-a5893346adc6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the_name_of_country': 'Iran',\n",
              " 'text': 'Persepolis Football Club and Esteghlal Football Club are two of the most popular and successful football teams in Iran. Both clubs have a rich history and a large fan base, and they have dominated the Iranian football scene, winning numerous domestic titles and representing Iran in continental competitions.'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke('Iran', return_only_outputs=True) ### این بار فقط تکست رو میاره"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2TLkzeDiBvB",
        "outputId": "ceffb9f9-f1c9-41c3-ddab-7eeebb60f8e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'The most well-known and successful football team in Iran is Persepolis Football Club, based in Tehran. They are one of the most popular and successful clubs in Iranian football history, having won numerous domestic titles and enjoying a large fan base.'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMMathChain  ### چین بعدی در ریاضیات\n",
        "\n",
        "llm_math = LLMMathChain.from_llm(llm, verbose=True)"
      ],
      "metadata": {
        "id": "yZt3eoS7iqz4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_math.prompt.template #### داره ممیگه خودش نمی تونه مساله ریاضی رو حل کنه از طریق تبدیل به کد پایتونی میتونه"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "H7Bcb51ukBqg",
        "outputId": "35cdc241-cdac-41c0-bf1c-833bb77829af"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "math_problems = [\"What is 13 raised to the .3432 power?\",\n",
        "                 \" Determine the result of raising 2 to the power of 10 and then subtracting the square root of 81.\"]\n",
        "# llm_math.invoke()\n",
        "llm_math.invoke(math_problems[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPQiLbKTi6fp",
        "outputId": "0b6e002b-c6bb-4cc5-f61b-fd9d6d058f16"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "What is 13 raised to the .3432 power?\u001b[32;1m\u001b[1;3m```text\n",
            "13 ** 0.3432\n",
            "```\n",
            "...numexpr.evaluate(\"13 ** 0.3432\")...\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is 13 raised to the .3432 power?',\n",
              " 'answer': 'Answer: 2.4116004626599237'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "در صورتیکه اگه ازش میخواستم با مدل زبانی این کار و کنه نتیجش میشه جواب پایین"
      ],
      "metadata": {
        "id": "0Ya67o4skP4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage\n",
        "llm.invoke([HumanMessage(content=math_problems[i])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBXGeFhFkWcK",
        "outputId": "81d3f87e-ba48-463d-b2bd-6be5a76d9bb2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='13 raised to the .3432 power is approximately **2.337**.', additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '5cf19df9-1d9c-4e31-a98c-7c90f97f24f7', 'token_count': {'input_tokens': 216.0, 'output_tokens': 20.0}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '5cf19df9-1d9c-4e31-a98c-7c90f97f24f7', 'token_count': {'input_tokens': 216.0, 'output_tokens': 20.0}}, id='run-5087b9e5-a30b-4e2b-bb29-1da4493760c7-0', usage_metadata={'input_tokens': 216, 'output_tokens': 20, 'total_tokens': 236})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "این مطلب داخل پرانتزیه و ربطی به سرفصل چین نداره:\n",
        "دیدن توکن ها در فارسی و انگلیسی : در فارسی فرق داره با توسعه مدل های زمانی طمان و تعداد توکن کمتر شده ولی در زبان انگلیسی فرقی نکرده\n",
        "\n",
        "https://platform.openai.com/tokenizer"
      ],
      "metadata": {
        "id": "8KmD2XZvk2Ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template =\"\"\"\n",
        " Please translate the following word from {input_language} to {output_language}\n",
        " word: {word}\n",
        " Answer: ?\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"input_language\", 'output_language', 'word'])\n"
      ],
      "metadata": {
        "id": "UutebOw1cLgC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. مدل نوشتن چین به صورت پایین"
      ],
      "metadata": {
        "id": "e4zN36eCeEJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt | llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSqE_pZIcYI7",
        "outputId": "09dd7de9-8520-451d-c32c-7f5b05d187ef"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['input_language', 'output_language', 'word'], input_types={}, partial_variables={}, template='\\n Please translate the following word from {input_language} to {output_language}\\n word: {word}\\n Answer: ?')\n",
              "| ChatCohere(client=<cohere.client.Client object at 0x78ad0d9f3c10>, async_client=<cohere.client.AsyncClient object at 0x78ad0d9f2710>, cohere_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator_chain = prompt | llm ### چینو اینجا ساختیم دیگه نیازی به پیش فرض های لنگ چین نداریم\n",
        "translator_chain.invoke({'input_language':'Farsi', 'output_language':'Spanish', 'word':\"راضیه\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EviS6RB1cd_S",
        "outputId": "6e685dc0-c54a-4aa2-8068-f555e51a30b5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='La palabra \"راضیه\" en español es \"Radiah\".', additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'c4d1eceb-f7f3-493d-af88-a0af995f6534', 'token_count': {'input_tokens': 222.0, 'output_tokens': 14.0}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'c4d1eceb-f7f3-493d-af88-a0af995f6534', 'token_count': {'input_tokens': 222.0, 'output_tokens': 14.0}}, id='run-873c4fbf-40b5-4846-a471-6e51435db5b9-0', usage_metadata={'input_tokens': 222, 'output_tokens': 14, 'total_tokens': 236})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "وقتی مدل زبانی از نوع چت بات باشه، ای آی مسیجی هست که رلش همینه و کانتنت داره"
      ],
      "metadata": {
        "id": "5EPcUF1JfLp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator_chain.invoke({'input_language':'Farsi', 'output_language':'Spanish', 'word':\"راضیه\"}).content ### برخلاف کد قبلی فقط استرینگ میده"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NIWuAq8Ucn4d",
        "outputId": "7de392d4-6b0e-4de8-fdba-c8f3c585cce1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'La palabra \"راضیه\" en español es \"Radia\".'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "با این عبارت فقط استرینگ را خواهیم داشت\n",
        "StrOutputParser\n"
      ],
      "metadata": {
        "id": "PbORnobAc9Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n"
      ],
      "metadata": {
        "id": "KX4x2HG2c6S6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | StrOutputParser() ### با این عبارت فقط استرینگ را خواهیم داشت"
      ],
      "metadata": {
        "id": "S3-VUD92cylB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "prompt = PromptTemplate(template='answer {question} honestly', input_variables=['question'])\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "chain.invoke({'question':'Which Iraninan Football Team is the best team from technical side and being famouse, please name one which is the best?'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "lD6LTwYqdLaP",
        "outputId": "7ae4cace-057c-452b-9a40-c4ff7300a8d7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As of my cut-off date, January 2024, the most successful Iranian football team is Persepolis Football Club. They have won a record 15 Persian Gulf Pro League titles, 7 Hazfi Cups, and 4 Iranian Super Cups. They are also the only Iranian team to have reached the final of the Asian Club Championship twice, in 1999 and 2020.\\n\\nPersepolis is known for its strong technical and tactical approach to the game, with a focus on possession-based football and a well-organized defense. The team has produced many talented players who have gone on to represent the Iranian national team. They have a large and passionate fan base, not only in Iran but also among the Iranian diaspora around the world.\\n\\nIn addition to their domestic success, Persepolis has also gained recognition in Asian football. They have regularly participated in the AFC Champions League, reaching the knockout stages on several occasions. The club's success and popularity have made them one of the most well-known and respected teams in Asian football.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "شبیه بالا اینو هم داریم ولی خیلی کابردی نیست"
      ],
      "metadata": {
        "id": "7TNXh5BGft3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# با این هم میشه استرینگ گرفت\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "parser = JsonOutputParser()\n",
        "prompt = PromptTemplate(template='answer {question} honestly with {format_instructions}', input_variables=['question'], partial_variables={\"format_instructions\": parser.get_format_instructions()})\n",
        "### برای کنترل خروجی برای نوع خروجی متغیر نوشت\n",
        "\n",
        "chain = prompt | llm | parser\n",
        "chain.invoke({'question':'who is Amirkabir?'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0fpioSydMB0",
        "outputId": "a225b230-8e7c-4afb-d90e-db21b35cffdb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'who_is_amirkabir': \"Amirkabir, also known as Mirza Ali Muhammad Nasir al-Din Shah, was a prominent Persian figure in the 19th century. He was a talented engineer, statesman, and reformer who played a significant role in the modernization of Iran during the Qajar dynasty. Amirkabir served as the prime minister of Iran from 1848 to 1851 under King Nassereddin Shah. He is renowned for his contributions to the development of Iran's infrastructure, education, and military. Amirkabir founded the first modern technical school in Iran, which is now known as Amirkabir University of Technology, one of the country's top universities. He is considered a national hero in Iran for his efforts to improve the country's technological and scientific advancements.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "اگر در کد بالا نگاه کنی، در قسمت تمپلیت متغیر رو برای قسمت پارشیال همون موقع مقدار دهی کردیم، هدف در این قسمت این بوده که بدانی می توانی متغیرهایی رو از هومن اول مقداردهی کنی که در این قسمت قابل نوشتن بوده"
      ],
      "metadata": {
        "id": "G_fAs-C2jBCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "پدل پایین رو هم خودم نوشتم و خودم خروجی گفتم که خروجی جیسون بده"
      ],
      "metadata": {
        "id": "iiHKWTN8j1Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "prompt = PromptTemplate(template='answer {question} honestly and give the answer with a json format', input_variables=['question'])\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "chain.invoke({'question':'who is Amirkabir?'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "AZVdos0Igavl",
        "outputId": "d665b656-b544-4240-8b9a-f155447c5f06"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n    \"name\": \"Amirkabir\",\\n    \"full_name\": \"Mirza Mohammad Taghi Khan Amir-Nezam\",\\n    \"title\": \"Amir Kabir\",\\n    \"birth_date\": \"1807\",\\n    \"birth_place\": \"Hazaraji, near Tabriz, Iran\",\\n    \"death_date\": \"January 10, 1852\",\\n    \"death_place\": \"Kasherat Fin, Kashan, Iran\",\\n    \"occupation\": \"Prime Minister of Iran\",\\n    \"known_for\": \"Being a reformer and founder of Dar ol-Fonoon, the first modern university in Iran\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "parser = JsonOutputParser()\n",
        "prompt = PromptTemplate(template='answer {question} honestly. \\n{format_instructions}\\n',\n",
        "                        input_variables=['question'], partial_variables={\"format_instructions\": parser.get_format_instructions()})\n",
        "\n",
        "chain = prompt | llm | parser\n",
        "# chain.invoke({'question':'who was Amirkabir?'})\n",
        "chain.invoke({'question':'who is Amirkabir?'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SGNvTDTjygN",
        "outputId": "76e2498a-f288-4477-9787-19dc513971d0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'who_is_amirkabir': 'Amirkabir, also known as Mirza Mohammad Taghi Khan Amir-Nezam, was a prominent Persian figure in the 19th century. He was a talented military commander, politician, and engineer during the Qajar dynasty in Iran. Amirkabir is widely regarded as one of the most influential reformers in Iranian history.\\n\\nBorn in 1807, Amirkabir rose to prominence due to his exceptional administrative and engineering skills. He served as the prime minister of Iran under Naser al-Din Shah Qajar from 1848 to 1851. During his tenure, he implemented various modernizing reforms, including the establishment of the first modern college in Iran, the Dar ul-Funun, which played a significant role in promoting Western science and technology in the country.\\n\\nAmirkabir was also responsible for numerous infrastructure projects, such as the construction of roads, bridges, and the famous Bazaar-e Amir in Tehran. He introduced the first postal system in Iran and played a crucial role in the modernization of the Iranian military.\\n\\nDespite his remarkable contributions, Amirkabir faced political rivalries and was eventually removed from power and executed in 1852. His legacy is remembered for his dedication to the advancement of Iran and his efforts to bring about progressive changes during a critical period in Iranian history.'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "چین ها هم نیز مانند مدل های زبانی،\n",
        "\n",
        "invoke\n",
        "\n",
        "batch\n",
        "\n",
        "chunk\n",
        "....\n",
        "دارند"
      ],
      "metadata": {
        "id": "PFCZeGM8l5N-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "مثال:\n",
        "\n",
        "پینی برای تراپیست"
      ],
      "metadata": {
        "id": "QchWRfiBmFTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "prompt = PromptTemplate(template='''You are a compassionate and understanding therapist. Please respond to the user's {question} s in an empathetic and helpful way''', input_variables=['question'])\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "# chain.invoke({'question':'I feel really anxious about my future. Because I want to apply for Generative AI in Spain for an european project, I dont know to be sad or happy, this subject is my favourite subject, but the sth I am worried about is that this project is a bit out of my league, althogh My professor accepted me, What should I do?'})\n",
        "chain.invoke({'question':'I feel really anxious about my future. I was accepted for PhD Position for a European Horizon 2020 Project and I donot know whether go or not go, the project is related to Generative AI in software engineering, this project is alittle bit away out of my league, in addition I dont know Spain is a good country to spend my phd position time or not?'})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "7yaJJ8XHkpZb",
        "outputId": "627f688e-9520-4203-eced-c2f81687b05d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I can understand that making decisions about your future, especially when it involves a significant move and a challenging project, can be incredibly anxiety-inducing. It's natural to feel a sense of apprehension when faced with such an important choice. Let's explore your concerns and try to ease some of your worries.\\n\\nFirst of all, congratulations on being accepted for the PhD position! It's an achievement to be recognized for your potential in such a competitive field. I sense that you might be feeling imposter syndrome, doubting your abilities to take on this project. It's common to feel this way, but remember that the selection committee saw your capabilities and the potential you bring to the table. Trust in their decision and your own skills. If they believe in you, you can start believing in yourself too.\\n\\nNow, regarding the project being a bit out of your league, it's important to remember that a PhD is a learning process. It's an opportunity to grow and expand your knowledge and expertise. It's okay to feel challenged; in fact, that's often where the most significant learning occurs. You'll have supervisors and a support system to guide you through the process, and you'll learn and develop new skills along the way. Embrace the opportunity to push yourself and step out of your comfort zone.\\n\\nAs for Spain, moving to a new country can be both exciting and daunting. It might help to consider the following aspects:\\n- **Cultural Experience:** Spain has a rich culture, and living there can offer you a unique and immersive experience. You'll have the chance to learn a new language, discover new traditions, and possibly make friends from diverse backgrounds.\\n- **Academic Environment:** Research the institution and the research group you'll be working with. Look into their reputation, the resources they offer, and the support system in place for PhD students. A supportive academic environment can make a big difference in your overall experience.\\n- **Quality of Life:** Spain is known for its beautiful cities, vibrant culture, and friendly people. Research the city where you'll be based, and consider factors like cost of living, accommodation options, and accessibility to amenities. A comfortable living environment can contribute to a positive PhD experience.\\n- **Networking and Opportunities:** Being part of a European Horizon 2020 Project will provide excellent networking opportunities and potential collaborations with experts in your field. This can open doors for your future career, regardless of where you ultimately decide to settle.\\n\\nRemember, it's normal to feel anxious about such a big decision. Take some time to weigh the pros and cons, and perhaps make a list of your concerns and the potential benefits. You could also try reaching out to current or past PhD students from the program to get their insights and perspectives. Their advice might help you make a more informed decision.\\n\\nIf you feel comfortable, you could also share your thoughts with a trusted friend or family member. Sometimes, talking it out can help clarify your own thoughts and feelings. Remember, this decision is about your happiness and growth, so consider what aligns best with your long-term goals and aspirations.\\n\\nYou have an exciting opportunity ahead of you, and I encourage you to believe in yourself and your abilities. If you decide to take on this challenge, remember that support is available every step of the way.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "میخوام شرایطی رو ایجاد کنیم که اگر هر شرظی برقرار شد، اون پرامپت رو ران کنه\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rCGYpnoQS6S6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### دو تا چین متفاوت میسازم برای تراپی و زبان برنامه نویسی\n",
        "\n",
        "model_parser = llm | StrOutputParser()\n",
        "\n",
        "### terapist chain\n",
        "terapist_prompt = PromptTemplate(template=\"You are a helpful therapy assistant. Please answer {question}.\", input_variables=[\"question\"])\n",
        "terapist = terapist_prompt | model_parser\n",
        "\n",
        "### code developer chain\n",
        "code_developer_prompt = PromptTemplate(template='you are an excellent {programming_language} developer. please answer {question}', input_variables=['programming_language', 'question'])\n",
        "code_developer = code_developer_prompt | model_parser"
      ],
      "metadata": {
        "id": "BEjU4bHRmVkt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### general chain\n",
        "general_chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"Respond to the following question:Question: {question} Answer:\"\"\"\n",
        "    )\n",
        "    | model_parser\n",
        ")\n"
      ],
      "metadata": {
        "id": "Mr0KDQzjTPZk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "در اینجا از\n",
        "\n",
        " RunnableBranch\n",
        "\n",
        "  استفاده شده است که این کلاس به شما این امکان را می‌دهد که بر اساس شرایط مختلف، یکی از چندین چین مختلف را اجرا کنید\n",
        "  \n",
        "  . RunnableBranch\n",
        "  \n",
        "   به این صورت کار می‌کند که یک لیست از جفت‌های\n",
        "   (شرط, runnable)\n",
        "      دریافت می‌کند، و اگر شرط برقرار باشد، مربوط به آن\n",
        "     runnable\n",
        "      اجرا می‌شود. در غیر این صورت، چین پیش‌فرض اجرا می‌شود.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HHNZx8XYPM1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableBranch, RunnableParallel\n",
        "### A RunnableBranch is initialized with a list of (condition, runnable) pairs and a default runnable.\n",
        "#### بررسی شرایط ران شدن یکی از چین ها\n",
        "branch = RunnableBranch(\n",
        "    (lambda x: \"terapy\" ==x[\"topic\"].lower(), terapist),\n",
        "    (lambda x: \"code\" in x[\"topic\"].lower(), code_developer),\n",
        "    general_chain\n",
        ")"
      ],
      "metadata": {
        "id": "RDTxzOFBTSNc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "در اینجا، ما یک دیکشنری از سه کلید داریم:\n",
        "\n",
        "\"topic\": از ورودی x مقدار topic را استخراج می‌کند.\n",
        "\"question\": از ورودی x مقدار question را استخراج می‌کند.\n",
        "\"programming_language\": از ورودی x مقدار code (در صورتی که وجود داشته باشد) را استخراج می‌کند، اگر نه مقدار None باز می‌گرداند.\n",
        "سپس این دیکشنری با | branch ترکیب می‌شود. این ترکیب به این معناست که پس از استخراج این مقادیر از ورودی، خروجی به RunnableBranch داده می‌شود تا مطابق با شرایط، چین مناسب انتخاب شود."
      ],
      "metadata": {
        "id": "XC8TeqjsOkxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain = {\"topic\": lambda x: x[\"topic\"],\n",
        "              \"question\": lambda x: x[\"question\"], \"programming_language\":lambda x: x.get('code', None)} | branch\n",
        "\n"
      ],
      "metadata": {
        "id": "Yv5-p15ETUzz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# مثلا ممکنه ووردی کاربر این باشه\n",
        "x = {\n",
        "    \"topic\": \"code\",\n",
        "    \"question\": \"How do I reverse a list in Python?\",\n",
        "    \"code\": \"python\"\n",
        "}\n",
        "# به خاطر همین از lambda استفاده شده"
      ],
      "metadata": {
        "id": "hErAZNEjTTBo"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "یعنی از کاربر ابتدا تاپیک و سوال رو میگیره، بعد میبره میده به فول چین، فول چین تاپیک و سوال رو استخراج میکنه و  میبره  تو شرایط ران ایبل، هر کدوم که برقرار بود اونو ران میکنه"
      ],
      "metadata": {
        "id": "M17o61iDPjxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({'topic': 'weather', 'question': 'what is the weather like in Barcelona in FebFebruary?'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "8nwllnBOTW5q",
        "outputId": "788d2880-14d3-4fd6-c804-2f9c3bd651a1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In February, Barcelona experiences mild and relatively comfortable weather compared to many other European cities. The climate is typically transitional between winter and spring during this month. Here's a general overview of what you can expect:\\n\\n**Temperature:** February is one of the cooler months in Barcelona, with average high temperatures ranging from 14°C to 16°C (57°F to 61°F) and average low temperatures between 6°C to 8°C (43°F to 46°F). However, it's not uncommon for temperatures to reach around 20°C (68°F) on warmer days.\\n\\n**Sunshine:** Barcelona enjoys a good amount of sunshine in February, with an average of 5-6 hours of sunshine per day. This makes it a pleasant time for sightseeing and outdoor activities.\\n\\n**Rainfall:** February is one of the drier months in Barcelona, but it still experiences some rainfall. The city receives an average of 40-50 mm of precipitation, with around 7-9 rainy days throughout the month. Rain often comes in short bursts, and it's not unusual to have sunny spells in between.\\n\\n**Humidity:** Humidity levels in Barcelona during February are moderate, ranging from 50% to 80%, which can make the cooler temperatures feel slightly chillier.\\n\\n**Wind:** February can be a bit windy, with breezes coming in from the Mediterranean Sea. Wind speeds can vary, but they generally contribute to a refreshing atmosphere.\\n\\n**Sea Temperature:** For those interested in water activities, the Mediterranean Sea's temperature around Barcelona in February is quite cool, averaging around 13°C to 14°C (55°F to 57°F).\\n\\nOverall, February in Barcelona offers a pleasant break from colder climates and is an excellent time to explore the city without the peak season crowds. It's advisable to pack layers of clothing to accommodate the varying temperatures throughout the day.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({'topic': 'code', 'question': 'how to fine tunning a LLM?'}) ### نباید کار کنه چون اسم زبان برنامه نویسی بهش داده نشده"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "tg8v-0FZTY5T",
        "outputId": "c140d095-330e-4e44-ace0-576dff4cf9a4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As an AI language model, I don't have personal experiences or preferences, but I can provide you with a comprehensive guide on how to fine-tune a Large Language Model (LLM). Fine-tuning is a crucial process in training LLMs to perform specific tasks or adapt to a particular domain. Here's an overview of the steps involved:\\n\\n**1. Prepare the Data:**\\n   - Identify the Task: Determine the specific task you want the LLM to perform, such as text classification, sentiment analysis, question-answering, or language generation.\\n   - Gather Training Data: Collect a large and diverse dataset relevant to your task. This data should represent the types of inputs and outputs you expect the model to handle. For example, it could be a collection of labeled sentences for text classification or question-answer pairs for a question-answering system.\\n\\n   - Preprocess the Data: Clean and preprocess the data to ensure consistency and remove any irrelevant information. This step may involve tokenization, lowercasing, removing punctuation, handling special characters, and converting the data into a suitable format for the model.\\n\\n   - Split the Data: Divide your dataset into training, validation, and test sets. The training set is used to train the model, the validation set is used for tuning hyperparameters and preventing overfitting, and the test set is used for final evaluation.\\n\\n**2. Choose a Pre-trained Language Model:**\\n   - Select a pre-trained LLM as the starting point for fine-tuning. Popular choices include models like GPT-3, T5, BERT, RoBERTa, or any other model available through platforms like Hugging Face's Transformers library. These models have been pre-trained on vast amounts of text and can be adapted to various downstream tasks.\\n\\n**3. Set Up the Fine-tuning Process:**\\n   - Choose a Fine-tuning Technique: There are several approaches to fine-tuning LLMs:\\n     - Full Model Fine-tuning: This involves updating all the parameters of the pre-trained model during training. It requires more computational resources but can lead to better performance.\\n     - Adapter-based Fine-tuning: Adapters are small additional layers inserted into the pre-trained model, allowing you to train only these new layers while keeping the original model's parameters frozen. This method is more parameter-efficient.\\n     - Prompt-based Fine-tuning: This technique involves learning a prompt or a set of instructions to guide the pre-trained model to generate desired outputs.\\n\\n   - Define Loss Function and Optimizer: Select an appropriate loss function that measures the discrepancy between the model's predictions and the true labels. Common choices include cross-entropy loss for classification tasks or mean squared error for regression. Choose an optimizer like Adam or SGD to update the model's parameters during training.\\n\\n   - Set Hyperparameters: Determine the learning rate, batch size, number of training epochs, and other hyperparameters. These values can significantly impact the fine-tuning process and model performance.\\n\\n**4. Fine-tune the Model:**\\n   - Train the model using the prepared training data and the chosen fine-tuning technique. During training, the model learns to adjust its parameters to minimize the loss function.\\n\\n   - Monitor Validation Performance: Regularly evaluate the model's performance on the validation set to detect overfitting and adjust hyperparameters accordingly.\\n\\n**5. Evaluate and Iterate:**\\n   - After fine-tuning, assess the model's performance on the test set using appropriate evaluation metrics for your task.\\n   - Analyze the results and iterate on the fine-tuning process if necessary. This may involve collecting more data, adjusting hyperparameters, or experimenting with different fine-tuning techniques.\\n\\n**6. Deployment and Inference:**\\n   - Once you are satisfied with the model's performance, you can deploy it to a suitable environment for inference. This could be a local server, a cloud platform, or an edge device, depending on your use case.\\n\\n**7. Continuous Improvement:**\\n   - Fine-tuning LLMs is an iterative process. You can continue to improve the model by collecting more data, experimenting with different pre-trained models, or exploring advanced fine-tuning techniques like domain-adaptive pre-training or multi-task learning.\\n\\nRemember that fine-tuning LLMs requires a solid understanding of machine learning, deep learning, and natural language processing. It's essential to have the necessary computational resources and a well-defined task to achieve the best results. Additionally, consider the ethical implications of your fine-tuned model and ensure it aligns with responsible AI practices.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({'topic': 'code', 'question': 'how to fine tunning a LLM?'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "F-I2xLyPpfBj",
        "outputId": "3c09f27c-6579-4863-e2b3-54aeae670276"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I appreciate the compliment, but I don't possess the ability to perform development tasks or write code. However, I can provide you with a general overview of the process of fine-tuning a Large Language Model (LLM):\\n\\n**1. Prepare your data:** Fine-tuning an LLM typically requires a large, high-quality dataset. The data should be relevant to the specific task or domain you want the model to learn. Clean and preprocess the data to ensure it is in a suitable format for training. This might involve tokenization, removing punctuation, handling special characters, and converting the data into a numerical representation that the model can understand.\\n\\n**2. Choose a base model:** Select a pre-trained LLM as your starting point. Popular choices include models like GPT-3, T5, BERT, or any of their variants. These models have already been trained on vast amounts of text and have learned general language patterns and representations.\\n\\n**3. Set up your fine-tuning environment:** You'll need a powerful computing infrastructure to fine-tune an LLM. This can be a local machine with high-end GPUs or a cloud-based solution provided by companies like AWS, Google Cloud, or Azure, which offer pre-configured environments for training large models.\\n\\n**4. Define your training objectives:** Clearly define the task you want the model to perform after fine-tuning. This could be a specific natural language processing (NLP) task such as text classification, sentiment analysis, named entity recognition, or a more general goal like improving the model's performance on a particular domain or style of text.\\n\\n**5. Fine-tuning process:**\\n   - **Model Architecture:** Decide whether you need to modify the base model's architecture to suit your specific task. In some cases, you might add or remove layers, adjust hyperparameters, or incorporate task-specific components.\\n\\n   - **Training:** Split your dataset into training and validation sets. Use the training set to update the model's parameters and the validation set to monitor its performance during training. You'll typically use a technique called 'transfer learning' where you freeze some of the base model's layers and only update the weights of the newly added or task-specific layers.\\n\\n   - **Optimization and Loss Function:** Choose an appropriate optimization algorithm (e.g., Adam, Adagrad) and a loss function that aligns with your task. For example, cross-entropy loss for classification tasks or mean squared error for regression.\\n\\n   - **Training Loop:** Iterate over your training data in batches, forward-pass the data through the model, calculate the loss, and then backpropagate the gradients to update the weights.\\n\\n   - **Evaluation:** Regularly evaluate the model's performance on the validation set to ensure it is learning effectively and to avoid overfitting.\\n\\n**6. Post-processing and Deployment:** After fine-tuning, you can further optimize the model by pruning unnecessary parameters or applying quantization techniques to reduce model size and inference time. Finally, deploy the fine-tuned model to a suitable environment where it can be integrated into your application or service.\\n\\nRemember that fine-tuning LLMs can be computationally expensive and time-consuming, and it often requires a good understanding of deep learning and NLP techniques. It's also crucial to consider the ethical implications of your fine-tuning process and ensure that the data and model usage adhere to responsible AI practices.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "حالا میتونیم شرایط رو به نحوی ببریم، که برنامه تاپیک رو از ورودی نگیره، و خودش براساس تشخیصی که میده ببره تو دسته بندی ها و اونجا جواب بده"
      ],
      "metadata": {
        "id": "DTXeXsBMQMOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "template = \"\"\"Just classify the topic of the following question: {question} into one of these three categories:\n",
        " ['code', 'therapy', 'general'] with just one category without any explanation.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template,\n",
        "                        input_variables=['question'])\n",
        "classifier_chain = prompt | model_parser\n"
      ],
      "metadata": {
        "id": "2nBdAbXyTa2H"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_chain.invoke({'question': 'my favorite football team is Perpolis of Iran'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "m6WyfAS_TdiU",
        "outputId": "745e8cca-5059-4ea9-9837-69132b19cc8a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'general'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "حالا تو این چین، تاپیک و با یه چینی تشخیص داده، سوال رو هم که از کاربر گرفته و مقدار زبان برنامه نویسی در صورت موجود، بعد میده به برنچ بالایی تا براساس برقراری هر شرط اونو اجرا کنه"
      ],
      "metadata": {
        "id": "-cra3kkJRsY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "final_chain = ({'topic': classifier_chain, 'question': itemgetter(\"question\"), 'programming_language': lambda x: x.get(\"language\",None)} | branch)\n"
      ],
      "metadata": {
        "id": "RP9T5-ojTfqr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain.invoke({ 'question': 'how to install numpy?', 'programming_language': 'Python'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "KXBfKS09Thcy",
        "outputId": "4a3df0bf-5fad-45c8-aa8a-d28e5a27cf93"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As an AI language model, I don't possess the ability to install software or packages on a computer system. However, I can provide you with instructions on how to install NumPy, a fundamental package for scientific computing in Python.\\n\\nNumPy can be installed using Python's package manager, pip. Here are the steps to install NumPy:\\n\\n1. Open your command prompt or terminal.\\n\\n2. Ensure you have Python installed and pip (Python's package installer) is up to date by running the following commands:\\n```\\npython --version\\npip install --upgrade pip\\n```\\n\\n3. Install NumPy by running the following command:\\n```\\npip install numpy\\n```\\n\\nThis will download and install the latest stable version of NumPy and its dependencies.\\n\\nIf you prefer to install NumPy using Anaconda or other Python package managers, you can find more detailed instructions on the official NumPy website: [https://numpy.org/install/](https://numpy.org/install/)\\n\\nRemember to check the compatibility of your Python version with the NumPy package to ensure a smooth installation process.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain.invoke({ 'question': 'What is the difference between a list and a tuple?', 'programming_language': 'python'})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "WtcJbO3_Tj2e",
        "outputId": "15e19b53-c4ed-483b-abb7-0431a68ed63e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As an AI language model, I don't have a preference for any programming language, but I can certainly explain the difference between a list and a tuple in Python.\\n\\n**List:**\\n- A list is a mutable (changeable) ordered sequence of elements.\\n- It is created using square brackets `[]` and can contain elements of different data types.\\n- Lists can be modified, meaning you can add, remove, or change elements after creation.\\n- Lists are more flexible and suitable for situations where data needs to be frequently updated.\\n\\nExample:\\n```python\\nmy_list = [1, 'apple', True, 3.14]\\nmy_list.append('orange')\\nprint(my_list)  # Output: [1, 'apple', True, 3.14, 'orange']\\n```\\n\\n**Tuple:**\\n- A tuple is an immutable (unchangeable) ordered sequence of elements.\\n- It is created using parentheses `()` and can also contain elements of different data types.\\n- Once a tuple is created, you cannot modify it. Tuples are useful for situations where data needs to remain constant.\\n- Tuples are faster to process than lists due to their immutable nature.\\n\\nExample:\\n```python\\nmy_tuple = (1, 'banana', False)\\n# my_tuple[0] = 10 # This would raise an error as tuples are immutable\\n```\\n\\nIn summary, the key difference is that lists are mutable and flexible, while tuples are immutable and provide a constant data structure. The choice between the two depends on the specific requirements of your program.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chatprompt template"
      ],
      "metadata": {
        "id": "LscAwswYGMQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "میخواهیم همین طور که قبلا تو نوشتن تمپلیتها و اینوکشون با\n",
        "\n",
        "llm\n",
        "\n",
        "چت پرامچت تمپلیت درست می کردیم که نقش بدیم، و مکانی برای سیو پیامها داشتیم در چین هم ایجاد کنیم"
      ],
      "metadata": {
        "id": "7aEjv8OaGZKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import SystemMessage, HumanMessage,AIMessage, StrOutputParser\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n"
      ],
      "metadata": {
        "id": "1IV-vRNkTlqy"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"user\", \"{text}\"),\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "YhO2pj4bG2-9"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | StrOutputParser()\n"
      ],
      "metadata": {
        "id": "hl0lqiFjG79a"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history=[]"
      ],
      "metadata": {
        "id": "XrJTS_fEG9zo"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "\n",
        "# texts = [\"I am a LLM specialist who is an expert in LangChain\",\n",
        "#          \"What is my area of expertise?\",\n",
        "#          \"\"\"Please provide some questions that the LLM (Language Model) may not be proficient at answering,\n",
        "#           but Wikipedia would be well-suited to address.\"\"\",\n",
        "#          \"Who won the Asian National Cup in 2023?\",\n",
        "#          \"I want to record an 80-second video to introduce my course about LangChain and its application. Why should we learn it? Why is it important to learn? Who should learn it?\",\n",
        "#          \"No, my friend, LangChain is a framework to work with LLMs. It's not about blockchain or similar topics. Please rewrite your previous answer.\",\n",
        "#          \"You did well, but consider the absolute beginner and start with some basic information. Maybe they do not know LLMs, but they know ChatGPT. So use this fact and adjust your response accordingly.\"\n",
        "#         ]\n",
        "\n",
        "texts = [\n",
        "    \"What is LangChain and how is it used with LLMs?\",\n",
        "    \"What are the primary components of LangChain?\",\n",
        "    \"How does LangChain handle prompt management in LLM applications?\",\n",
        "    \"Can you explain how LangChain enables memory and chaining in workflows?\",\n",
        "    \"If I want to build an advanced LLM-based application using LangChain, how can I utilize prompt management and memory together?\"\n",
        "]\n",
        "\n",
        "\n",
        "text = texts[4]\n",
        "# chat_history = chat_history[-3:] میگه 3 تای آخر و نگه دار\n",
        "response_message = chain.invoke({'chat_history':chat_history, 'text':text})\n",
        "response_message"
      ],
      "metadata": {
        "id": "akYnEGVKHAoV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b3bb4c44-338e-468a-a28e-8c5f5df8b93d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Combining prompt management and memory effectively is a powerful approach when building advanced LLM-based applications using LangChain. Here's how you can utilize these two concepts together:\\n\\n## Prompt Management:\\n- **Prompt Templates:** Create well-designed prompt templates that guide the LLM to generate responses in the desired format. Include placeholders or variables in the prompt to dynamically insert context-specific information. For example, you might have a template for asking questions, providing instructions, or presenting context.\\n- **Conditional Prompting:** Implement conditional logic in your prompts to adapt the instructions or context based on the specific needs of each interaction. This can involve checking the current state, previous responses, or external data to determine what information to include in the prompt.\\n- **Prompt Optimization:** Experiment with different prompt variations and techniques to find the most effective way to elicit relevant and accurate responses from the LLM. A/B testing and iterative refinement of prompts can lead to improved performance.\\n- **User Input Processing:** If your application accepts user input, design prompts that can effectively instruct the LLM to generate responses based on user queries or commands. This might involve natural language understanding techniques to interpret user intent.\\n\\n## Memory Management:\\n- **Chain of Thought (CoT):** LangChain's CoT feature is essential for maintaining memory. Create a chain of tasks or steps where each step builds upon the previous one. This chain can be used to keep track of the conversation history, intermediate calculations, or any relevant information that the LLM should consider for its next response.\\n- **Vector Stores and Embeddings:** Utilize LangChain's vector databases (also known as embedding stores) to store and retrieve relevant pieces of information. You can save important context, facts, or user-specific data as embeddings in the vector store. When generating a response, retrieve similar or related embeddings to provide additional context to the LLM.\\n- **State Management:** Implement a state management system to keep track of variables, user preferences, or any dynamic data that might influence the LLM's response. Update and store the state after each interaction, ensuring that the LLM has access to the most recent context.\\n- **Memory Retrieval Strategies:** Experiment with different memory retrieval techniques, such as selecting the most recent relevant memories, aggregating multiple memories, or using attention mechanisms to weigh the importance of different pieces of information.\\n\\n## Integrating Prompt and Memory:\\n- **Contextual Prompts:** Combine the power of prompts and memory by creating dynamic prompts that include relevant memory content. For instance, when generating a response, retrieve previous conversation turns or related facts from the memory and incorporate them into the prompt.\\n- **Iterative Question Answering:** Implement a process where the LLM generates a response, and then you use that response to update the memory and create a new prompt for further clarification or additional questions. This can lead to more accurate and contextually rich interactions.\\n- **Feedback Loop:** Establish a feedback loop where the LLM's output is used to update the memory, which, in turn, influences the prompts for subsequent interactions. This ensures that the LLM learns and adapts based on its previous responses.\\n- **Error Handling and Redundancy:** Use memory to handle errors or ambiguous responses. If the LLM provides an incorrect or incomplete answer, store this information in the memory and design prompts to encourage the model to revisit and correct its previous output.\\n\\nBy integrating prompt management and memory effectively, you can create LLM-based applications that provide coherent, contextually aware, and informative responses. LangChain's flexibility allows developers to customize and experiment with these techniques to suit the specific requirements of their applications.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "تو این قسمت هر کدوم از سوالای بالا رو اضافه کنی و جوابی که چین میده رو توی اینجا میریزه و گفته هر شخصی یا کانتنی که تولید میکنه مشخصه"
      ],
      "metadata": {
        "id": "5pqspwlLKwmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.extend([HumanMessage(content=text), AIMessage(content=response_message)])\n",
        "chat_history"
      ],
      "metadata": {
        "id": "-Ghu2nHnHEYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0923e8b1-9cab-472b-c3f3-56ca274f35eb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='What is LangChain and how is it used with LLMs?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"LangChain is a framework and set of tools designed to help developers build applications that utilize Large Language Models (LLMs) more effectively, particularly for natural language processing tasks. It was created by Harrison Chase to make working with LLMs more accessible and efficient.\\n\\nHere's an overview of LangChain and its use with LLMs:\\n1. **Language Model Abstraction:** LangChain provides a standardized way to interact with various LLMs by abstracting their differences. It allows developers to write code that can work with different language models without needing to modify the underlying logic for each model. This abstraction layer simplifies the process of switching between models like GPT-3, GPT-4, or other LLM providers.\\n2. **Chain of Thought and Memory:** One of the key features of LangChain is its ability to create and manage chains of thought. It enables developers to break down complex tasks into smaller steps, providing a structured approach to problem-solving. LangChain can maintain and retrieve relevant information from previous steps, effectively creating a 'memory' for the LLM, which is crucial for context-aware responses.\\n3. **Data Retrieval and Integration:** LangChain facilitates the integration of external data sources with LLMs. It allows developers to connect databases, knowledge bases, or documents to the LLM, enabling the model to retrieve and utilize relevant information during text generation. This is especially useful for providing factual and up-to-date responses.\\n4. **Prompt Engineering and Templates:** LangChain offers tools for prompt engineering, which involves designing prompts or instructions for LLMs to generate desired outputs. It includes prompt templates, prompt programming, and techniques to guide the LLM's response format and content.\\n5. **Applications:** LangChain is used in building various applications, such as question-answering systems, conversational agents, content generation tools, and automated document processing systems. It enhances the LLM's capabilities by providing structure, context, and access to external knowledge sources.\\n6. **Open-Source Community:** LangChain is an open-source project, which means developers can contribute to its development, share ideas, and create custom extensions. This fosters a community-driven approach to improving LLM applications.\\n\\nIn summary, LangChain serves as a bridge between developers and LLMs, making it easier to build sophisticated language-based applications by providing memory, context, and data integration capabilities. It is a powerful toolkit that has gained popularity in the natural language processing community for its ability to enhance the functionality and efficiency of LLMs.\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='If I want to build an advanced LLM-based application using LangChain, how can I utilize prompt management and memory together?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Combining prompt management and memory effectively is a powerful approach when building advanced LLM-based applications using LangChain. Here's how you can utilize these two concepts together:\\n\\n## Prompt Management:\\n- **Prompt Templates:** Create well-designed prompt templates that guide the LLM to generate responses in the desired format. Include placeholders or variables in the prompt to dynamically insert context-specific information. For example, you might have a template for asking questions, providing instructions, or presenting context.\\n- **Conditional Prompting:** Implement conditional logic in your prompts to adapt the instructions or context based on the specific needs of each interaction. This can involve checking the current state, previous responses, or external data to determine what information to include in the prompt.\\n- **Prompt Optimization:** Experiment with different prompt variations and techniques to find the most effective way to elicit relevant and accurate responses from the LLM. A/B testing and iterative refinement of prompts can lead to improved performance.\\n- **User Input Processing:** If your application accepts user input, design prompts that can effectively instruct the LLM to generate responses based on user queries or commands. This might involve natural language understanding techniques to interpret user intent.\\n\\n## Memory Management:\\n- **Chain of Thought (CoT):** LangChain's CoT feature is essential for maintaining memory. Create a chain of tasks or steps where each step builds upon the previous one. This chain can be used to keep track of the conversation history, intermediate calculations, or any relevant information that the LLM should consider for its next response.\\n- **Vector Stores and Embeddings:** Utilize LangChain's vector databases (also known as embedding stores) to store and retrieve relevant pieces of information. You can save important context, facts, or user-specific data as embeddings in the vector store. When generating a response, retrieve similar or related embeddings to provide additional context to the LLM.\\n- **State Management:** Implement a state management system to keep track of variables, user preferences, or any dynamic data that might influence the LLM's response. Update and store the state after each interaction, ensuring that the LLM has access to the most recent context.\\n- **Memory Retrieval Strategies:** Experiment with different memory retrieval techniques, such as selecting the most recent relevant memories, aggregating multiple memories, or using attention mechanisms to weigh the importance of different pieces of information.\\n\\n## Integrating Prompt and Memory:\\n- **Contextual Prompts:** Combine the power of prompts and memory by creating dynamic prompts that include relevant memory content. For instance, when generating a response, retrieve previous conversation turns or related facts from the memory and incorporate them into the prompt.\\n- **Iterative Question Answering:** Implement a process where the LLM generates a response, and then you use that response to update the memory and create a new prompt for further clarification or additional questions. This can lead to more accurate and contextually rich interactions.\\n- **Feedback Loop:** Establish a feedback loop where the LLM's output is used to update the memory, which, in turn, influences the prompts for subsequent interactions. This ensures that the LLM learns and adapts based on its previous responses.\\n- **Error Handling and Redundancy:** Use memory to handle errors or ambiguous responses. If the LLM provides an incorrect or incomplete answer, store this information in the memory and design prompts to encourage the model to revisit and correct its previous output.\\n\\nBy integrating prompt management and memory effectively, you can create LLM-based applications that provide coherent, contextually aware, and informative responses. LangChain's flexibility allows developers to customize and experiment with these techniques to suit the specific requirements of their applications.\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tOVIoi8JHFKi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}